
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
%\documentclass[conference]{IEEEtran}
\documentclass[10pt, conference, compsocconf]{IEEEtran}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}


% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi




% *** CITATION PACKAGES ***
\usepackage{url}
\usepackage{cite}
\usepackage[numbers,square, comma, sort&compress]{natbib}
%\usepackage[comma]{natbib}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% \cite{http://www.ctan.org/tex-archive/macros/latex/contrib/cite/}
% The documentation is contained in the cite.sty file itself.


\usepackage[T1]{fontenc}


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithm2e}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/
\usepackage{algorithm} %format of the algorithm
\usepackage{algorithmic} %format of the algorithm
\usepackage{multirow} %multirow for format of table
\usepackage{amsmath}
\usepackage{xcolor}
%\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\setlength{\textfloatsep}{5pt plus 1.0pt minus 2.0pt}

% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


\usepackage{mdwmath}
\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.
\usepackage{graphicx}
\usepackage{subfloat}
\usepackage{float}

\usepackage{caption}
%\captionsetup[table]{belowskip=18pt, aboveskip = 5pt}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\nocite{*}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Bayesian Filter based Object Traction in Computer Vision}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Liang Xu}
	\IEEEauthorblockA{Electrical and Computer Enineering\\
		virginia commonwealth university\\
}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
%
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3},
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
	%\boldmath
As most widely used in cell phone and other areas, camera be come the most widely used sensor. Also, there are lots of information contains in one or a serial of picture. This project will focus on Bayesian based tracking in computer vision. For tracking, there are two areas, one is tracking the object in a serial image, another areas is tracking the camera's location. The first case most called object tracking, and the second is called  Simultaneous Localization and Mapping(SLAM). \\
Particle filter based tracking is widely used in computer vision field. Tracking is the problem of generating an inference about the motion of an object given a sequence of images. Generally, we will have some measurements that appear at each tick of clock. These measurements could be the position of some image points, the position and moments of some image regions, or pretty much anything else. These are not guaranteed to be relevant, in the sense that some could come from other objects or from noise. We will have an encoding of the object's state and some model of how this state change from thick to thick. We would like to infer the state of the world from the measurement and the model of dynamics.  \\
The past decades have been significant progress in robot navigation and Simultaneous Localization and Mapping(SLAM).  SLAM is the computational problem of constructing a map of an unknown environment on the fly while simultaneously keeping track of a moving object with a sensor on it.  It has been widely used in the robotic community. Different sensors can contribute their own knowledge with estimating the environment. \\
Accelerator, IMU, GPS and many another sensors could be used for SLAM. However, many surprising reasons vision is an attractive choice for SLAM sensor: cameras are compact, accurate, informative,  cheap and ubiquitous.  Vision also is the primary navigation tool for human and animals.  So this project will use the Extended Kalmen Filter(EKF) knowledge learned from the class and Computer Vision to build a real-time visual SLAM system. 
\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the conference you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals/conferences frown on
% math in the abstract anyway.


%% no keywords
%\vspace{1em}
%
%\noindent \textbf{Keywords:}\quad Flash memory, Out-of-place update, Read cache, LRU.
% \vspace{1em}




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\section{Introduction}
Tracking problems are of grate practical importance. There are very good reasons to want to, track aircraft  using radar returns. Motions capture, if we can track the 3D configuration of a moving person accurately, the we can make an accurate record of their motions. Once we have this record, then we can use it to drive a rendering process. Recognition from motion: the motion of objects is quite characteristic. We might be able to determine the identity of the object from its motion. Tracking is not only for track the object in the image or video, it also include to track the camera's location. The Visual-SLAM is the area for this topic. \\
The real-time V-SLAM have two parts, one is the computer vision and another is the parameter estimation. The computer vision can give a better knowledge of the image get by the camera, and parameter estimation can help us to estimate the current position. \\
So, how to estimate the current point by just using the image? First, we are assuming the camera is moving, and it is a dynamic movement. This means the movement is a continuous function, and follow the Newton's Law. Second, there are Natural Visual Landmarks on each image. This marks could be some special points or corners in the image. There is the certain case, we can not get any feature points from the image, but we are not going to discuss that case. According to the first assumption, the object movement is a continuous function, so there are strong relation between the images. So we can use the position different between to image to estimate the position of the camera. This the whole idea of this project. 
\section{Particle Filter based Object Tracking} 
In this section will focus on how to track the object in the image or video. In this section, I am going to implement tracking method for image sequence and videos. The main algorithm used in this section is particle filter. 
\subsection{Particle Filter Algorithm}
Below is the particle filter procedure. 
\begin{itemize}
\item Object is the thing that is actually being tracked.
\item x(t) represent the state of the model at time t.
\item A dynamic model p(x(t)|x(t-1)) distribution of the state at time t given the state at t - 1.
\item A measurement z(t) that somehow captures the data of the current image. 
\item A sensor model p(z(t)|x(t)) that gives the likelihood of a measurement given the state. For Bayesian-based tracking, we assume that tome t -1 we have a believe about the state. For represented as a density p(x(t-1)), and that given some measurements z(t) at time t we update our Belief by computing the following equations Figure \ref{fig:bel}. 
\end{itemize}
\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.30]{image/bel.png}
	\caption{Belief Updating}
	\label{fig:bel}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.50]{image/part_update.png}
	\caption{Particle Filter Updating}
	\label{fig:part_update}
\end{figure}

In this project, the problem will track an image patch template taken from the first frame of the video. For this project, the object is simply going to be the image patch, and the state will be only the 2D center location of the patch. Thus each particle will be a (u,v) pixel location(where u and v are the row and column number respective) representing a proposed location for the center of the template window equal size in the current image, the Mean Squared Error as Figure \ref{fig:mse}. The funny indexing is just because (u, v) are indexed from 1 to M (or N) but the state <u,v> is the location of the center.\\
MSE, of course, only indicates how dissimilar the image patch is, whereas we need a similarity measure so that more similar patches are more likely. Thus, we will use a squared exponential equation (Gaussian) to convert this into a usable measurement function Figure \ref{fig:measure}: 

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.20]{image/mse.png}
	\caption{Mean Square Error for Template}
	\label{fig:mse}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.30]{image/measure.png}
	\caption{Measurement Function }
	\label{fig:measure}
\end{figure}

\subsection{Basic Particle Filter}

In this problem, we implement a basic particle filter to a simple object Figure \ref{fig:OBJ}. Figure \ref{fig:P1_000} is the first picture. The object is doing a nonlinear motion. When the object touch the boundary, it turns the direction. So the particle's variance increased after touch the right boundary, like Figure \ref{fig:P1_026} 

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.50]{image/template1.png}
	\caption{Tracking Object }
	\label{fig:OBJ}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.30]{image/P1_000.jpg}
	\caption{First Image 000}
	\label{fig:P1_000}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.30]{image/P1_0015.jpg}
	\caption{First Image 015}
	\label{fig:P1_015}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.30]{image/P1_0026.jpg}
	\caption{First Image 026}
	\label{fig:P1_026}
\end{figure}

\subsection{Changes in Appearance}
in the last section we were working with a static template, assuming that the target will not change in shape. This can be addressed by updating the model's appearance over time. In this case, we need include a step to use the history to update the tracking window model. We can accomplish this using an Infinite Impulse Response(IIR) filter. The concept
is simple: we first find the best tracking window for the current particle distribution as displayed in the
visualizations. Then we just update the current window model to be a weighted sum of the last model
and the current best estimate. 
\[Template = \alpha Best(t) + (1-\alpha)Template(t-1)\]
where Best(t) is the patch of the best estimate or mean estimate. It's easy to see that by recursively
updating this sum, the window implements an exponentially decaying weighted sum of (all) the past
windows.\\
For this part, we are going to track governor Romney's hand. Because his hand is moving, so each particle will 4 states which are u, v, u speed, and v speed. Here are the result Figure \ref{fig:000},\ref{fig:006},\ref{fig:022},\ref{fig:164}. Figure \ref{fig:000} is the initial update, we can easily check all the particle are every where across the whole image. And we can check the hand is change the shape as well. As the hand change too quickly, the particle have a hard time to catch the movement, as Figure \ref{fig:164}, the particle's variance increased. 

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.20]{image/000.jpg}
	\caption{P2\_000}
	\label{fig:000}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.20]{image/006.jpg}
	\caption{P2\_006}
	\label{fig:006}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.20]{image/022.jpg}
	\caption{P2\_022}
	\label{fig:022}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.20]{image/164.jpg}
	\caption{P2\_164}
	\label{fig:164}
\end{figure}

\subsection{Occlusions}
For this part we will work with a much more difficult video to perform tracking with, we would like to be able to track the blond-haired woman (the one with the white jacket) as she crosses the road. If you try applying your adaptive tracker to this video, you will probably find that you will have difficulty dealing simultaneously with occlusion and the perspective shift as the woman walks away from the camera. Thus, we need some way of relying less on updating our appearance model from previous frames and more on a more sophisticated model of the dynamics of the figure we want to track. \\
For this problem, I modify my state to [x, y, Velocity\_x, Velocity\_Y, Frame\_Scale]. So, this problem have 5 state variables. X and Y are position for center of the tracked object.  Velocity\_x and Velocity\_Y can help me to build the dynamic motion model for the tracked object. One important change is the Frame\_scale, this variable will track size of the object. The tracking template will change according to the particle variance and the error between the current object and saved template.\\ 
The template will stop updating if the current object have a large error, which means the occlusion happened.  
During the occlusion, the particle will move according to its dynamic motion, because the occlusion happened at middle of the tracking, so the particleâ€™s dynamic model has been already build by filter out the other unfitted model. Therefor during the occlusion, the template will stop updating, and the particles are moving according to its own dynamic motion model.\\
Figure \ref{fig:Occlusions000} \ref{fig:p3_135} \ref{fig:p3_158} \ref{fig:p3_198} \ref{fig:p3_318} are the results pictures. Figure \ref{fig:Occlusions000} is the initial image from this video, compare with the Figure \ref{fig:p3_318}, the size of the figure is changing. And in the middle of the tracking, there are tow times for occlusion happened. From Figure \ref{fig:p3_135} we can easily find out the variance of the particle get larger. Figure \ref{fig:p3_158} is the image after the first occlusion, we can see the variance become smaller. Also the size of the object changed during the tracking. 
\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.50]{image/p3/000.jpg}
	\caption{Occlusions\_000}
	\label{fig:Occlusions000}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.50]{image/p3/135.jpg}
	\caption{Occlusions\_135}
	\label{fig:p3_135}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.50]{image/p3/158.jpg}
	\caption{Occlusions\_158}
	\label{fig:p3_158}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.50]{image/p3/198.jpg}
	\caption{Occlusions\_198}
	\label{fig:p3_198}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.50]{image/p3/318.jpg}
	\caption{Occlusions\_318}
	\label{fig:p3_318}
\end{figure}

\subsection{Tracking Multiple Targets}
In this section, I will investigate more on multiple object tracking. Figure \ref{fig:multi} are the three objects, in this problem, the object appear at different time and vanish at different time also. Figure \ref{fig:multi001}, \ref{fig:multi012}, \ref{fig:multi017}, \ref{fig:multi036} are the results. Multi-object tracking is pretty easy, just have multiple particle filter for each object.  
\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.30]{image/multi.png}
	\caption{Multi Objects}
	\label{fig:multi}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.30]{image/p4/000001.jpg}
	\caption{Multi Objects 001}
	\label{fig:multi001}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.30]{image/p4/000012.jpg}
	\caption{Multi Objects 012}
	\label{fig:multi012}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.30]{image/p4/000017.jpg}
	\caption{Multi Objects 017}
	\label{fig:multi017}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.30]{image/p4/000036.jpg}
	\caption{Multi Objects 036}
	\label{fig:multi036}
\end{figure}

\section{Simultaneous Localization and Mapping(SLAM)}
\subsection{Feature Point Extraction}
There are whole chapter and papers on the feature point extraction. So I am not going to discuss too much on this topic. Figure \ref{fig:featurePoint} is one of the examples of feature points, the red circle in the image are the feature points. Scale-invariant feature transform(SIFT) is the most common used feature points extraction technique, and the feature points are also called key points. SIFT is a robust algorithm to scale, transform and rotation. So the key-point find in the current frame has a high probability in the next frame also.\\
Feature matching also another topic. However, in this report, we only use this technique. Each of the key points will have its own descriptor, and the matting algorithm will search the descriptor for the target image. The expected movement of those key point is not too far from each other. So according to the sampling time, we can estimate the movement of the camera.
\begin{figure}[h]
	\centering
	\includegraphics[scale = 0.60]{image/featurePoint.png}
	\caption{Feature Point of an Image}
	\label{fig:featurePoint}
\end{figure}
\subsection{Camera Position Transformation}
Before estimate the camera position, we need to know how the 3D sense project to 2D image. The work flow is this: 3D sense project to 2D image using camera, estimate the position and movement using the key-point in 2D image, estimate the 3D position of the camera in the world coordinate, and then build a 3D map. \\
For the camera, there are intrinsic and extrinsic parameters Those parameters are fixed by the manufacturer. Figure \ref{fig:camera} is the example of transform the world coordinate to the camera coordinate and also the image coordinate.
\begin{figure}[h]
	\centering
	\includegraphics[scale = 0.60]{image/camera.png}
	\caption{Camera Parameter}
	\label{fig:camera}
\end{figure}
\subsection{Camera Position Estimation}
With the knowledge of the coordinate transform and feature points information, we can plug in the Extented Kalman Filter to estimate the position of camera in world coordinate. Figure \ref{fig:cameraMotion} is a simulated image for camera position estimation. Figure \ref{fig:state} and \ref{fig:states} explain the states of the camera. Figure \ref{fig:stateupdate} are the equation to update the camera states. \\
The map is initialized at system start-up and persists until operation ends, but evolves continuously and dynamically as it is updated by the Extended Kalman Filter. The probabilistic state estimates of the camera and features are updated during camera motion and feature observation. When new features are observed the map is enlarged with new states and, if necessary, features can also be deleted.\\
The probabilistic character of the map lies in the propagation over time not only of the mean best estimates of the states of the camera and features but a first order uncertainty distribution describing the size of possible deviations from these values. Mathematically, the map is represented by a state vector as figure \ref{fig:state}.  State vector x is composed of the stacked state estimates of the camera and features and P is a square matrix of equal dimension which can be partitioned into submatrix elements. \\
In doing this, the probability distribution over all map parameters is approximated as a single multivariate Gaussian distribution in a space of dimension equal to the total state vector size.
\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.60]{image/cameraMotion.png}
	\caption{Camera Parameter}
	\label{fig:cameraMotion}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.60]{image/state.png}
	\caption{Camera state}
	\label{fig:state}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.50]{image/states.png}
	\caption{Camera state represent}
	\label{fig:states}
\end{figure}
\subsection{Motion Modeling and Prediction} 
After start-up, the state vector is updated in two alternating ways: 1) the prediction step, when the camera moves in the blind interval between image capture and 2) the update step, after measurements have been achieved of features.\\
We assume that, in each time step, unknown acceleration aW and angular acceleration W processes of zero mean and Gaussian distribution cause an impulse of velocity and angular velocity.  
\begin{figure}[h]
	\centering
	\includegraphics[scale = 0.50]{image/stateupdate.png}
	\caption{Camera state update}
	\label{fig:stateupdate}
\end{figure}
\subsection{Real Time Tracking} 
Figure \ref{fig:slam} is the result using my desktop and web camera. 
\begin{figure}[h]
	\centering
	\includegraphics[scale = 0.40]{image/slam.png}
	\caption{Real time v-slam}
	\label{fig:slam}
\end{figure}

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
{\footnotesize \bibliographystyle{IEEEtran}
\bibliography{references}}

%\begin{thebibliography}{1}
%
%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%
%\end{thebibliography}




% that's all folks
\end{document}


